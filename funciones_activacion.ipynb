{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de activación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Sigmoide y softmax (clasificación) \n",
    "\n",
    "### Función Sigmoide\n",
    "\n",
    "La función sigmoide se define como:\n",
    "\n",
    "\\[\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\]\n",
    "\n",
    "Donde:\n",
    "- Convierte cualquier valor real \\(x\\) en un rango entre \\(0\\) y \\(1\\).\n",
    "- Es comúnmente utilizada en la última capa de redes neuronales para problemas de clasificación binaria.\n",
    "\n",
    "**Sigmoide**\n",
    "   - **Usos**:\n",
    "     - Problemas de clasificación binaria.\n",
    "     - Capas de salida en redes neuronales simples.\n",
    "   - **Ventajas**:\n",
    "     - Salida en rango \\([0, 1]\\), útil para probabilidades.\n",
    "   - **Limitaciones**:\n",
    "     - Problema del desvanecimiento del gradiente.\n",
    "     - Saturación en valores extremos.\n",
    "\n",
    "---\n",
    "\n",
    "### Función Softmax\n",
    "\n",
    "La función softmax se define como:\n",
    "\n",
    "\\[\n",
    "f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\n",
    "\\]\n",
    "\n",
    "Donde:\n",
    "- Convierte un vector de valores reales \\(x = [x_1, x_2, \\dots, x_n]\\) en una distribución de probabilidad.\n",
    "- Es utilizada en la última capa de redes neuronales para problemas de clasificación multiclase.\n",
    "\n",
    "**Softmax**\n",
    "   - **Usos**:\n",
    "     - Clasificación multiclase.\n",
    "     - Capas de salida en redes neuronales para problemas de clasificación.\n",
    "   - **Ventajas**:\n",
    "     - Convierte salidas en una distribución de probabilidad.\n",
    "   - **Limitaciones**:\n",
    "     - Sensible a valores extremos en las entradas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU (lineal) Para CNN y RNN\n",
    "\n",
    "### Función ReLU (Rectified Linear Unit)\n",
    "\n",
    "La función ReLU se define como:\n",
    "\n",
    "\\[\n",
    "f(x) = \\max(0, x)\n",
    "\\]\n",
    "\n",
    "Donde:\n",
    "- Si \\(x > 0\\), entonces \\(f(x) = x\\).\n",
    "- Si \\(x \\leq 0\\), entonces \\(f(x) = 0\\).\n",
    "\n",
    "Es ampliamente utilizada en redes neuronales, especialmente en CNN y RNN, debido a su simplicidad y capacidad para introducir no linealidad.\n",
    "\n",
    "**ReLU (Rectified Linear Unit)**\n",
    "   - **Usos**: \n",
    "     - Redes neuronales profundas (DNN), redes convolucionales (CNN) y redes recurrentes (RNN).\n",
    "   - **Ventajas**:\n",
    "     - Simplicidad computacional.\n",
    "     - Ayuda a mitigar el problema del desvanecimiento del gradiente.\n",
    "   - **Limitaciones**:\n",
    "     - Problema de \"neurona muerta\" (cuando los gradientes se vuelven cero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tanh\n",
    "\n",
    "### Función Tanh (Tangente Hiperbólica)\n",
    "\n",
    "La función Tanh se define como:\n",
    "\n",
    "\\[\n",
    "f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "\n",
    "**Tanh (Tangente Hiperbólica)**\n",
    "   - **Usos**:\n",
    "     - Redes neuronales recurrentes (RNN).\n",
    "     - Casos donde se necesita una salida centrada en cero.\n",
    "   - **Ventajas**:\n",
    "     - Salida en rango \\([-1, 1]\\), útil para datos normalizados.\n",
    "   - **Limitaciones**:\n",
    "     - Problema del desvanecimiento del gradiente."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
